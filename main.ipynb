{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting up Python Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt\n",
    "# %pip install ipykernel langchain_experimental llama-index-vector-stores-pinecone ipykernel PyMuPDF pinecone-client pypdf faiss-cpu langchain_community transformers sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import dotenv\n",
    "import faiss\n",
    "import fitz\n",
    "import huggingface_hub\n",
    "import langchain\n",
    "import langchain_community\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import pypdf\n",
    "import requests\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from llama_index.core import (SimpleDirectoryReader, StorageContext,\n",
    "                              VectorStoreIndex)\n",
    "from llama_index.core.extractors import (QuestionsAnsweredExtractor,\n",
    "                                         TitleExtractor)\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# import pinecone\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# # OpenAI API Key:\n",
    "# openai = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# # Pinecone API Key:\n",
    "# pinecone_api_key =os.getenv('PINECONE_API_KEY')\n",
    "# environment =os.getenv('PINECONE_ENV')\n",
    "\n",
    "# # Hugging Face Token:\n",
    "# HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# # configure Pinecone client\n",
    "# pc = Pinecone(api_key=pinecone_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys securely\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "print(\"API keys loaded securely.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 2 Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section A. Experimenting with Vector Store Query Design (50 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama_index.embeddings.huggingface\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import json, os, io, re, requests, fitz, dotenv, transformers, pinecone, pypdf, faiss, sqlite3, langchain_community, langchain, openai, math, time, nltk, torch, huggingface_hub, datasets\n",
    "import requests\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, Pinecone  \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys securely\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# configure Pinecone client\n",
    "pc = Pinecone(api_key=\"pcsk_549ncM_8EkcmEKkZ4MAHHRYzgVeKAa9hkHumAJ5K3G7yE4X7JYm6rgRLKT3sMdn34e5Muo\")\n",
    "\n",
    "print(\"API keys loaded securely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(\"the-word-2023-24-12.11.23.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Choose a method to chunk the text data:**\n",
    "\n",
    "- [Semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)\n",
    "\n",
    "- [Recursive chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
    "\n",
    "- [Character chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter)\n",
    "\n",
    "- [Token chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose a type of chunker (From langchain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# # parser to split up PDF resume:\n",
    "# text_parser = SentenceSplitter(\n",
    "#     chunk_size=1024\n",
    "# )\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load an embedding model for semantic chunking\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize the Semantic Chunker\n",
    "semantic_chunker = SemanticChunker(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "doc_idxs = []\n",
    "\n",
    "\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    page_text = page.get_text(\"text\")\n",
    "    cur_text_chunks = semantic_chunker.split_text(page_text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(text_chunks)} chunks parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc_idx = doc_idxs[idx]\n",
    "    src_page = doc[src_doc_idx]\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_chunks[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chunker Choices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunker choice #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunker choice #2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create text nodes from chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "        metadata={'text': text_chunk}  # type: ignore\n",
    "    )\n",
    "    src_doc_idx = doc_idxs[idx]\n",
    "    src_page = doc[src_doc_idx]  # pymupdf.Page\n",
    "    nodes.append(node)\n",
    "\n",
    "print(f'{len(nodes)} nodes created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",  # This is llama_index.llms.openai.OpenAI, not openai.AI\n",
    "             api_key=\"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\")\n",
    "\n",
    "\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=extractors,\n",
    ")\n",
    "nodes = await pipeline.arun(nodes=nodes, in_place=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create the vector store using chosen similarity metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API Key:\n",
    "openai = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# pinecone_api_key=\"pcsk_549ncM_8EkcmEKkZ4MAHHRYzgVeKAa9hkHumAJ5K3G7yE4X7JYm6rgRLKT3sMdn34e5Muo\"\n",
    "\n",
    "# Pinecone API Key:\n",
    "pinecone_api_key =os.getenv('PINECONE_API_KEY')\n",
    "environment =os.getenv('PINECONE_ENV')\n",
    "\n",
    "# Hugging Face Token:\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# configure Pinecone client\n",
    "pc = Pinecone(api_key= \"pcsk_549ncM_8EkcmEKkZ4MAHHRYzgVeKAa9hkHumAJ5K3G7yE4X7JYm6rgRLKT3sMdn34e5Muo\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_serverless = os.environ.get(\"USE_SERVERLESS\", \"False\").lower() == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    "\n",
    "# specify the Pinecone environment to use:\n",
    "#if use_serverless:\n",
    "    #spec = pinecone.ServerlessSpec(cloud='aws', region=\"us-east-1\")\n",
    "#else:\n",
    "   # spec = pinecone.PodSpec(environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name our Pinecone Index:\n",
    "index_name = \"hw02\"\n",
    "\n",
    "# If a Pinecone index of the same name already exists, delete it:\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **choose a similarity metric to use for the vector store:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define similarity and additional parameters for the vector store index:\n",
    "dimensions = 1536 #364  #1536 #768              # the dimensions of the index need to align with the LLM we are using for the RAG system. For example, if using openAI then dimenion = 1536. If using Llama2, then dimension = 384.\n",
    "\n",
    "# \"dotproduct\" is one similarity metric we can for the vector store index. We can use different distance metrics to measure the similarity between vector embeddings and user queries. This is where we define what similarity metric we are going to use for the vector store.\n",
    "# \"cosine\" is another similarity metric we can use for the vector store index.\n",
    "# \"euclidean\" is another similarity metric we can use for the vector store index.\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name, \n",
    "    dimension=dimensions, \n",
    "    metric=\"cosine\",          # we can use different distance metrics to measure the similarity between vector embeddings and user queries. this is where we define what similarity metric we are going to use for the vector store.\n",
    "    spec=spec\n",
    ")\n",
    "\n",
    "# wait for index to be ready before connecting\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "   time.sleep(1)\n",
    "\n",
    "for index in pc.list_indexes():\n",
    "    print(index['name'])\n",
    "\n",
    "\n",
    "pc.describe_index(\"hw02\")\n",
    "\n",
    "\n",
    "pc_index = pc.Index(index_name)  # create an index to use in the vector store\n",
    "\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=pc_index)    # this function creates a vector store where we will add and store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_index = pc.Index(index_name)  # create an index to use in the vector store\n",
    "vector_store = PineconeVectorStore(pinecone_index=pc_index)    # this function creates a vector store where we will add and store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc_index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# # extractors = [\n",
    "# #     TitleExtractor(nodes=5, llm=llm),\n",
    "# #     QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "# # ]\n",
    "\n",
    "# # pipeline = IngestionPipeline(\n",
    "# #     transformations=extractors,\n",
    "# # )\n",
    "# # nodes = await pipeline.arun(nodes=nodes, in_place=False)\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# extractors = [\n",
    "#     TitleExtractor(nodes=5, llm=llm),\n",
    "#     QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "# ]\n",
    "\n",
    "# pipeline = IngestionPipeline(\n",
    "#     transformations=extractors,\n",
    "# )\n",
    "# nodes = await pipeline.arun(nodes=nodes, in_place=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***choose an embedding model to use for the vector store:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OpenAI Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada=\"text-embedding-ada-002\"\n",
    "# small_txt_embedmodel_=\"text-embedding-3-small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", api_key = \"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\")\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Node embedding dimension is {len(nodes[0].embedding)}')  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Initialize OpenAI embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Apply embeddings to nodes\n",
    "for node in nodes:\n",
    "    node.embedding = embed_model.get_text_embedding(node.get_text())\n",
    "\n",
    "# Now, add nodes with embeddings to Pinecone\n",
    "vector_store.add(nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **load the embeddings into the vector store (e.g. create a vector store):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc_index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieve Content from the Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Directly pass the API key\n",
    "client = OpenAI(api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the query:\n",
    "# query = (\n",
    "#     \"Where are pets allowed on CMU?\"\n",
    "# )\n",
    "\n",
    "# # choose one of these models:\n",
    "# embed_model_ada = \"text-embedding-ada-002\"\n",
    "# embed_model_3_small = \"text-embedding-3-small\"\n",
    "\n",
    "# res = client.embeddings.create(\n",
    "#     input=[query],\n",
    "#     model= embed_model_ada \n",
    "# )\n",
    "\n",
    "# # retrieve from Pinecone\n",
    "# xq = res.data[0].embedding #res['data'][0]['embedding']\n",
    "\n",
    "# # get relevant contexts (including the questions)\n",
    "# res2 = pc_index.query(vector=xq, top_k=2, include_metadata=True)\n",
    "\n",
    "# Define the queries\n",
    "k = 5\n",
    "queries = [\n",
    "    \"What is the policy statement for the academic integrity policy?\",\n",
    "    \"What is the policy violation definition for cheating?\",\n",
    "    \"What is the policy statement for improper or illegal communications?\",\n",
    "    \"What are CMU’s quiet hours?\",\n",
    "    \"Where are pets allowed on CMU?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "\n",
    "# Choose one of these models:\n",
    "embed_model_ada = \"text-embedding-ada-002\"\n",
    "embed_model_3_small = \"text-embedding-3-small\"\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    res = client.embeddings.create(\n",
    "        input=[query],\n",
    "        model=embed_model_3_small\n",
    "    )\n",
    "\n",
    "    # Retrieve from Pinecone\n",
    "    xq = res.data[0].embedding  # res['data'][0]['embedding']\n",
    "\n",
    "    # Get relevant contexts (including the questions)\n",
    "    res2 = pc_index.query(vector=xq, top_k=k, include_metadata=True)\n",
    "\n",
    "    # Add response results\n",
    "    responses.append(res2)\n",
    "    \n",
    "\n",
    "# # Choose an embedding model\n",
    "# embed_model = \"text-embedding-ada-002\"  # or \"text-embedding-3-small\"\n",
    "\n",
    "# # Generate embeddings for each query\n",
    "# res = client.embeddings.create(\n",
    "#     input=queries,\n",
    "#     model=embed_model\n",
    "# )\n",
    "\n",
    "# # Retrieve relevant contexts from Pinecone\n",
    "# query_embeddings = [item.embedding for item in res.data]  # Extract embeddings\n",
    "\n",
    "# # Query Pinecone index for each query\n",
    "# top_k = 5  # Number of relevant contexts to retrieve\n",
    "# results = [\n",
    "#     pc_index.query(vector=embedding, top_k=top_k, include_metadata=True)\n",
    "#     for embedding in query_embeddings\n",
    "# ]\n",
    "\n",
    "# # Print or process results\n",
    "# for i, query in enumerate(queries):\n",
    "#     print(f\"Query: {query}\")\n",
    "#     print(f\"Top {top_k} results: {results[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_results = []\n",
    "\n",
    "for q in range(len(queries)):\n",
    "    response = responses[q]\n",
    "    for k, match in enumerate(response.matches):\n",
    "        curr_result = {\n",
    "            'q': q+1,\n",
    "            'k': k+1,\n",
    "            'score': match.score,\n",
    "            'text': match.metadata.get('text', ''),\n",
    "            'document_title': match.metadata.get('document_title', '')\n",
    "        }\n",
    "        response_results.append(curr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(response_results).to_csv('parta_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results:\n",
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Query the vector store using these queries**\n",
    "\n",
    "**Instruction: set the 'k' parameter to 5**\n",
    "\n",
    "Query 1: What is the policy statement for the academic integrity policy?\n",
    "\n",
    "Query 2: What is the policy violation definition for cheating?\n",
    "\n",
    "Query 3: What is the policy statement for improper or illegal communications?\n",
    "\n",
    "Query 4: What are CMU’s quiet hours?\n",
    "\n",
    "Query 5: Where are pets allowed on CMU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***query the vector store with the 5 queries above (don't forget to record the responses in your homework submission spreadsheet: see instructions for a link to the spreadsheet!):***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the vector store with the 5 queries above (don't forget to record the responses in your homework submission!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.II.** Explain your rationale for choosing the similarity metric you decided to use in the vector store. What is one pro of using the metric, and what is one difference between using the metric you selected and the other two similarity metrics we discussed in the lab. (We discussed cosine, dot product, and euclidean similarity metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.III.** Copy and paste the results or information retrieved from the vector store in response to each of the queries you submitted to the vector store in the SPREADSHEET TEMPLATE (please see instructions for a link to the spreadsheet template you should copy and use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.IV.** Qualitatively analyze the responses to your queries submitted to the vector store. Did the queries retrieve the information you were expecting to obtain. Why or why not? Why do you think the queries were successful / unsuccessful in retrieving the information you expected or needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section B. Experimenting with Vector Store Embeddings & Query Parameters (50 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Choose 1 of the 5 queries provided in A.1.6.A, above, and experiment with submitting the query to the vector store by changing the search parameters in the following manner:\n",
    "\n",
    "\n",
    "*   A) Baseline query, e.g. query, k=1.\n",
    "\n",
    "*   B) Query, parameter k = 3\n",
    "\n",
    "*   C) Query, parameter k = 5\n",
    "\n",
    "*   D) Query, parameter k = 10\n",
    "\n",
    "**In your written homework submission, record the UNIQUE responses/results of each query submitted to the vector store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Return to step A.1.B., above, and select a different text chunking method (e.g. word, sentence, paragraph). \n",
    "- Chunk your text data using the method. Create embeddings for the text. \n",
    "- Load the embeddings into the vector store. \n",
    "- Submit the same query you selected in B.1, above, and submit it to the vector store 6 times (using the different ‘k’ parameter settings defined in B.1, above), and record the responses.\n",
    "\n",
    "**In your written homework submission, record the responses/results of each query submitted to the vector store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.I.** Explain your rationale for selecting the query you choose in B.1. Why did you choose this query vs. the other 4 queries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.II.** Copy and paste the responses to the queries you submitted to the vector store in the SPREADSHEET TEMPLATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.III.** Copy and paste the responses to the queries you submitted to the vector store in the SPREADSHEET TEMPLATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.IV.** In observing the responses from the vector store to the queries created in B.1., which ‘k’ parameter do you think retrieved the highest quality / most accurate result? Why do you think this parameter was the best to use with the query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.V.** In observing the responses from the vector store to the queries created in B.2., which ‘k’ parameter do you think retrieved the highest quality / most accurate result? Why do you think this parameter was the best to use with the query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BONUS TASKS / QUESTIONS: Define function to call LLM API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please email Sara for the Bonus Task Python Notebook once you've completed your homework assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
